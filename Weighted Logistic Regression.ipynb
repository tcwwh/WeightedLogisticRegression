{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stat\n",
    "\n",
    "class WeightedMultinomialLogisticRegression(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor for the homgrown Logistic Regression\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "        \n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.coef_ = None               # weight vector\n",
    "        self.intercept_ = None          # bias term\n",
    "        self._theta = None              # augmented weight vector, i.e., bias + weights\n",
    "                                        # this allows to treat all decision variables homogeneously\n",
    "        self.likelihood = None          # -2*log(L)\n",
    "        self.AIC = None                 # AIC\n",
    "        self.BIC = None                 # BIC\n",
    "        self.Rsquare = None             # deviance R square\n",
    "        self.adj_Rsquare = None         # adjusted deviance R square\n",
    "        self.z_scores = {}              # z-score for theta\n",
    "        self.p_values = {}              # p_values for theta\n",
    "        self.fisher_inf_matrix = {}     # fisher information matrix\n",
    "        self.sigma_estimates = {}       # sqaure root of covariance matrix for theta\n",
    "        self.lower = {}                 # lower confidence interval for theta\n",
    "        self.upper = {}                 # upper confidence interval for theta\n",
    "        self.history = {\"cost\": [], \n",
    "                        \"acc\": [], \n",
    "                        \"val_cost\":[], \n",
    "                        \"val_acc\": []}\n",
    "        \n",
    "    def _grad(self, X, y, weight):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the Logistic Regression \n",
    "        objective function\n",
    "\n",
    "        Args:\n",
    "            X(ndarray):      train objects\n",
    "            y(ndarray):      answers for train objects\n",
    "            weight(ndarray): weights for train objects for weighted logistic regression\n",
    "            \n",
    "        Return:\n",
    "            grad(ndarray): gradient\n",
    "        \"\"\"\n",
    "        # number of training examples\n",
    "        n = X.shape[0]\n",
    "        \n",
    "        # get scores for each class and example\n",
    "        # 2D matrix\n",
    "        scores = self._predict_raw(X)\n",
    "        \n",
    "        # transform scores to probabilities\n",
    "        # softmax\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \n",
    "        # error\n",
    "        probs[range(n),y] -= 1\n",
    "        \n",
    "        # gradient\n",
    "        gradient = np.dot(X.T, weight.reshape(n, 1)*probs) / n\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def _gd(self, X, y, weight, max_iter, eta, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Runs Full GD and logs error, weigths, gradient at every step\n",
    "\n",
    "        Args:\n",
    "            X(ndarray):      train objects\n",
    "            y(ndarray):      answers for train objects\n",
    "            weight(ndarray): weights for train objects for weighted logistic regression\n",
    "            max_iter(int):   number of weight updates\n",
    "            eta(float):      step size in direction of gradient\n",
    "            \n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for i in range(max_iter):\n",
    "            \n",
    "            metrics = self.score(X, y)\n",
    "            self.history[\"cost\"].append(metrics[\"cost\"])\n",
    "            self.history[\"acc\"].append(metrics[\"acc\"])\n",
    "            \n",
    "            if X_val is not None:\n",
    "                metrics_val = self.score(X_val, y_val)\n",
    "                self.history[\"val_cost\"].append(metrics_val[\"cost\"])\n",
    "                self.history[\"val_acc\"].append(metrics_val[\"acc\"])\n",
    "\n",
    "            # calculate gradient\n",
    "            grad = self._grad(X, y, weight)\n",
    "            \n",
    "            # do gradient step\n",
    "            self._theta -= eta * grad\n",
    "    \n",
    "    def fit(self, X, y, weight=None, max_iter=5000, eta=0.01, val_data=None):\n",
    "        \"\"\"\n",
    "        Public API to fit Logistic regression model\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      train objects\n",
    "            y(ndarray):      answers for train objects\n",
    "            weight(ndarray): wweights for train objects for weighted logistic regression\n",
    "            max_iter(int):   number of weight updates\n",
    "            alpha(float):    step size in direction of gradient\n",
    "            \n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Augment the data with the bias term.\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        if val_data is not None:\n",
    "            X_val, y_val = val_data\n",
    "            X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
    "        else:\n",
    "            X_val = None\n",
    "            y_val = None\n",
    "        # initialize if the first step\n",
    "        if self._theta is None:\n",
    "            self._theta = np.random.rand(X.shape[1], len(np.unique(y)))\n",
    "        #initialize the weight\n",
    "        if weight is None:\n",
    "            weight = np.ones(X.shape[0]) \n",
    "        \n",
    "        # do full gradient descent\n",
    "        self._gd(X, y, weight, max_iter, eta, X_val, y_val)\n",
    "        \n",
    "        # get final weigths and bias\n",
    "        self.intercept_ = self._theta[0, :]\n",
    "        self.coef_ = self._theta[1:, :]\n",
    "        \n",
    "        #calculate the various statistics\n",
    "        self._statistics_(X, y, weight)\n",
    "\n",
    "    \n",
    "    def _statistics_(self, X, y, w):\n",
    "        \"\"\"\n",
    "        calculate the various statistics for the model\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      objects\n",
    "            y(ndarray):      answers for objects\n",
    "            w(ndarray):      weights for objects\n",
    "            \n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        #calculate the various model diagnostic statistics\n",
    "        #the code is from \n",
    "        #https://gist.github.com/rspeare/77061e6e317896be29c6de9a85db301d\n",
    "        n = X.shape[0]\n",
    "        p = X.shape[1]\n",
    "        \n",
    "        for i in range(np.unique(y).shape[0]):\n",
    "            denom = (2.0*(1.0+np.cosh(self.decision_function(X)[:, i])))\n",
    "            denom = np.tile(denom,(p,1)).T\n",
    "            fisher_inf_matrix = np.dot((X/denom).T,X*w[:, np.newaxis]) ## Fisher Information Matrix\n",
    "            Cramer_Rao = np.linalg.inv(fisher_inf_matrix) ## Inverse Information Matrix\n",
    "            sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n",
    "            \n",
    "            z_scores = self._theta[:, i]/sigma_estimates # z-score for eaach model coefficient\n",
    "            p_values = [stat.norm.sf(abs(x))*2 for x in z_scores] ### two tailed test for p-values\n",
    "\n",
    "            alpha = 0.05\n",
    "            q = stat.norm.ppf(1 - alpha / 2)\n",
    "            lower = self._theta[:, i] - q * sigma_estimates\n",
    "            upper = self._theta[:, i] + q * sigma_estimates\n",
    "\n",
    "            self.z_scores[i] = z_scores\n",
    "            self.p_values[i] = p_values\n",
    "            self.sigma_estimates[i] = sigma_estimates\n",
    "            self.fisher_inf_matrix[i] = fisher_inf_matrix\n",
    "            self.lower[i] = lower                \n",
    "            self.upper[i] = upper\n",
    "        \n",
    "        scores = self._predict_raw(X)\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \n",
    "        probs_null = np.bincount(y)/n\n",
    "        probs_null = np.tile(probs_null, (n, 1))\n",
    "        \n",
    "        self.likelihood = np.sum(np.log(probs[range(n),y])*w)\n",
    "        likelihood_null = np.sum(np.log(probs_null[range(n), y])*w)\n",
    "        self.Rsquare = 1 - self.likelihood/likelihood_null\n",
    "        self.adj_Rsquare = 1 - (self.likelihood/(n-p-1))/(likelihood_null/(n-1))\n",
    "        \n",
    "        self.AIC = 2*p - 2*self.likelihood\n",
    "        self.BIC = p*np.log(n) - 2*self.likelihood\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes logloss and accuracy for (X, y)\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      objects\n",
    "            y(ndarray):      answers for objects\n",
    "            \n",
    "        Return:\n",
    "            metrics(dict):   python dictionary which\n",
    "                             contains two fields: for accuracy \n",
    "                             and for objective function\n",
    "        \"\"\"\n",
    "        # number of training samples\n",
    "        n = X.shape[0]\n",
    "        \n",
    "        # get scores\n",
    "        scores = self._predict_raw(X)\n",
    "        \n",
    "        # trasnform scores to probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \n",
    "        # logloss per each example\n",
    "        corect_logprobs = -np.log(probs[range(n),y])\n",
    "        \n",
    "        # total mean logloss\n",
    "        data_loss = np.sum(corect_logprobs) / n\n",
    "        \n",
    "        # predictions\n",
    "        pred = np.argmax(scores, axis=1)\n",
    "        # accuracy\n",
    "        acc = accuracy_score(y, pred)\n",
    "        \n",
    "        # final metrics\n",
    "        metrics = {\"acc\": acc, \"cost\": data_loss}\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _predict_raw(self, X):\n",
    "        \"\"\"\n",
    "        ...... each class and each object in X\n",
    "        calculate the raw score and substract the maximum to avoid overflow\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      objects\n",
    "        \n",
    "        Return:\n",
    "            ...(ndarray): ...... each class and object\n",
    "        \"\"\"\n",
    "        if X.shape[1] == len(self._theta):\n",
    "            scores = np.dot(X, self._theta)\n",
    "        else:\n",
    "            scores = np.dot(X, self.coef_) + self.intercept_\n",
    "        \n",
    "        scores -= np.max(scores, axis=1, keepdims=True)\n",
    "        \n",
    "        return scores    \n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        ...... each class and each object in X\n",
    "        calculate the raw score without substracing the maximum\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      objects\n",
    "        \n",
    "        Return:\n",
    "            ...(ndarray): ...... each class and object\n",
    "        \"\"\"\n",
    "        if X.shape[1] == len(self._theta):\n",
    "            scores_ = np.dot(X, self._theta)\n",
    "        else:\n",
    "            scores_ = np.dot(X, self.coef_) + self.intercept_\n",
    "        \n",
    "        return scores_\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class for each object in X\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      objects\n",
    "        \n",
    "        Return:\n",
    "            pred(ndarray):   class for each object\n",
    "        \"\"\"\n",
    "        # get scores for each class\n",
    "        scores = self._predict_raw(X)\n",
    "        # choose class with maximum score\n",
    "        pred = np.argmax(scores, axis=1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stat\n",
    "import pandas as pd\n",
    "\n",
    "class WeightedBinaryLogisticRegression(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor for Weighted Binary Logistic Regression\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "        \n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.coef_ = None               # weight vector\n",
    "        self.intercept_ = None          # bias term\n",
    "        self._theta = None              # augmented weight vector, i.e., bias + weights\n",
    "                                        # this allows to treat all decision variables homogeneously\n",
    "        self.likelihood = None          # -2*log(L)\n",
    "        self.AIC = None                 # AIC\n",
    "        self.BIC = None                 # BIC\n",
    "        self.Rsquared = None            # deviance R squared\n",
    "        self.adj_Rsquared = None        # adjusted deviance R squared\n",
    "        self.z_scores = None            # z_scores for theta\n",
    "        self.p_values = None            # p_values for theta\n",
    "        self.fisher_inf_matrix = None   # fisher information matrix                              \n",
    "        self.sigma_estimates = None     # sqaure root of covariance matrix for theta\n",
    "        self.lower = None               # lower confidence interval for theta\n",
    "        self.upper = None               # upper confidence interval for theta\n",
    "        self.history = {\"cost\": [], \n",
    "                        \"acc\": [], \n",
    "                        \"val_cost\":[], \n",
    "                        \"val_acc\": []}\n",
    "        \n",
    "    def _grad(self, X, y, weight):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the Logistic Regression \n",
    "        objective function\n",
    "\n",
    "        Args:\n",
    "            X(ndarray):      train objects\n",
    "            y(ndarray):      answers for train objects\n",
    "            weight(ndarray): weights for train objects for weighted logistic regression\n",
    "            \n",
    "        Return:\n",
    "            grad(ndarray): gradient\n",
    "        \"\"\"\n",
    "        # number of training examples\n",
    "        n = X.shape[0]\n",
    "        \n",
    "        # get scores for each class and example\n",
    "        scores = self._predict_raw(X)\n",
    "        \n",
    "        # transform scores to probabilities\n",
    "        probs = np.zeros(X.shape[0])\n",
    "        probs[(scores>=0).flatten()] = 1/(1+np.exp(-1*scores[scores>=0].flatten()))\n",
    "        probs[(scores<0).flatten()] = 1 - 1/(1+np.exp(scores[scores<0].flatten()))\n",
    "        \n",
    "        # error\n",
    "        probs[y == 1] -= 1\n",
    "        \n",
    "        # gradient\n",
    "        gradient = np.dot(X.T, (weight*probs).reshape(n, 1)) / n\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def _gd(self, X, y, weight, max_iter, eta, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Runs Full GD and logs error, weigths, gradient at every step\n",
    "\n",
    "        Args:\n",
    "            X(ndarray):      train objects\n",
    "            y(ndarray):      answers for train objects\n",
    "            weight(ndarray): weights for train objects for weighted logistic regression\n",
    "            max_iter(int):   number of weight updates\n",
    "            eta(float):      step size in direction of gradient\n",
    "            \n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for i in range(max_iter):\n",
    "            \n",
    "            metrics = self.score(X, y)\n",
    "            self.history[\"cost\"].append(metrics[\"cost\"])\n",
    "            self.history[\"acc\"].append(metrics[\"acc\"])\n",
    "            \n",
    "            if X_val is not None:\n",
    "                metrics_val = self.score(X_val, y_val)\n",
    "                self.history[\"val_cost\"].append(metrics_val[\"cost\"])\n",
    "                self.history[\"val_acc\"].append(metrics_val[\"acc\"])\n",
    "\n",
    "            # calculate gradient\n",
    "            grad = self._grad(X, y, weight)\n",
    "            \n",
    "            # do gradient step\n",
    "            self._theta -= eta * grad\n",
    "    \n",
    "    def fit(self, X, y, weight=None, max_iter=5000, eta=0.01, val_data=None):\n",
    "        \"\"\"\n",
    "        Public API to fit Logistic regression model\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      train objects\n",
    "            y(1darray):      answers for train objects\n",
    "            weight(ndarray): wweights for train objects for weighted logistic regression\n",
    "            max_iter(int):   number of weight updates\n",
    "            alpha(float):    step size in direction of gradient\n",
    "            \n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Augment the data with the bias term.\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        if val_data is not None:\n",
    "            X_val, y_val = val_data\n",
    "            X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
    "        else:\n",
    "            X_val = None\n",
    "            y_val = None\n",
    "        # initialize if the first step\n",
    "        if self._theta is None:\n",
    "            self._theta = np.random.rand(X.shape[1], 1)\n",
    "        #initialize the weight\n",
    "        if weight is None:\n",
    "            weight = np.ones(X.shape[0]) \n",
    "        \n",
    "        # do full gradient descent\n",
    "        self._gd(X, y, weight, max_iter, eta, X_val, y_val)\n",
    "        \n",
    "        # get final weigths and bias\n",
    "        self.intercept_ = self._theta[0]\n",
    "        self.coef_ = self._theta[1:]\n",
    "        \n",
    "        #calculate the various statistics\n",
    "        self._statistics_(X, y, weight)\n",
    "\n",
    "    \n",
    "    def _statistics_(self, X, y, w):\n",
    "        \"\"\"\n",
    "        calculate the various statistics for the model\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      objects\n",
    "            y(ndarray):      answers for objects\n",
    "            w(ndarray):      weights for objects\n",
    "            \n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        #calculate the various model diagnostic statistics\n",
    "        #the code is from \n",
    "        #https://gist.github.com/rspeare/77061e6e317896be29c6de9a85db301d\n",
    "        n = X.shape[0]\n",
    "        p = X.shape[1]\n",
    "        \n",
    "        denom = (2.0*(1.0+np.cosh(self.decision_function(X)))).flatten()\n",
    "        denom = np.tile(denom,(p, 1)).T\n",
    "        fisher_inf_matrix = np.dot((X/denom).T, X*w[:, np.newaxis]) ## Fisher Information Matrix\n",
    "        Cramer_Rao = np.linalg.inv(fisher_inf_matrix) ## Inverse Information Matrix\n",
    "        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n",
    "\n",
    "        z_scores = self._theta.flatten()/sigma_estimates # z-score for eaach model coefficient\n",
    "        p_values = [stat.norm.sf(abs(x))*2 for x in z_scores] ### two tailed test for p-values\n",
    "\n",
    "        alpha = 0.05\n",
    "        q = stat.norm.ppf(1 - alpha / 2)\n",
    "        lower = self._theta.flatten() - q * sigma_estimates\n",
    "        upper = self._theta.flatten() + q * sigma_estimates\n",
    "\n",
    "        self.z_scores = z_scores\n",
    "        self.p_values = p_values\n",
    "        self.sigma_estimates = sigma_estimates\n",
    "        self.fisher_inf_matrix = fisher_inf_matrix\n",
    "        self.lower = lower                \n",
    "        self.upper = upper\n",
    "        \n",
    "        probs = self.predict_probs(X) \n",
    "        \n",
    "        probs_null = np.bincount(y)/n\n",
    "        probs_null = np.tile(probs_null, (n, 1))\n",
    "        \n",
    "        self.likelihood = np.sum(np.log(probs[range(n),y])*w)\n",
    "        likelihood_null = np.sum(np.log(probs_null[range(n), y])*w)\n",
    "        self.Rsquared = 1 - self.likelihood/likelihood_null\n",
    "        self.adj_Rsquared = 1 - (self.likelihood/(n-p-1))/(likelihood_null/(n-1))\n",
    "        \n",
    "        self.AIC = 2*p - 2*self.likelihood\n",
    "        self.BIC = p*np.log(n) - 2*self.likelihood\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes logloss and accuracy for (X, y)\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      objects\n",
    "            y(ndarray):      answers for objects\n",
    "            \n",
    "        Return:\n",
    "            metrics(dict):   python dictionary which\n",
    "                             contains two fields: for accuracy \n",
    "                             and for objective function\n",
    "        \"\"\"\n",
    "        # number of training samples\n",
    "        n = X.shape[0]\n",
    "        \n",
    "        # get probabilities\n",
    "        probs = self.predict_probs(X)\n",
    "        \n",
    "        # logloss \n",
    "        data_loss = np.sum(-np.log(probs[range(n),y]))/n\n",
    "        \n",
    "        # predictions\n",
    "        pred = np.argmax(probs, axis=1)\n",
    "        # accuracy\n",
    "        acc = accuracy_score(y, pred)\n",
    "        \n",
    "        # final metrics\n",
    "        metrics = {\"acc\": acc, \"cost\": data_loss}\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _predict_raw(self, X):\n",
    "        \"\"\"\n",
    "        ...... each class and each object in X\n",
    "        calculate the raw score and substract the maximum to avoid overflow\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      objects\n",
    "        \n",
    "        Return:\n",
    "            ...(ndarray): ...... each class and object\n",
    "        \"\"\"        \n",
    "        if X.shape[1] == len(self._theta):\n",
    "            scores = np.dot(X, self._theta)\n",
    "        else:\n",
    "            scores = np.dot(X, self.coef_) + self.intercept_\n",
    "        \n",
    "        return scores   \n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        ...... each class and each object in X\n",
    "        calculate the raw score without substracing the maximum\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      objects\n",
    "        \n",
    "        Return:\n",
    "            ...(ndarray): ...... each class and object\n",
    "        \"\"\"\n",
    "        return self._predict_raw(X)\n",
    "    \n",
    "    def predict_probs(self, X):\n",
    "        \"\"\"\n",
    "        ...... each class and each object in X\n",
    "        calculate the raw score without substracing the maximum\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      objects\n",
    "        \n",
    "        Return:\n",
    "            ...(ndarray): ...... each class and object\n",
    "        \"\"\"\n",
    "        scores = self._predict_raw(X)\n",
    "        probs = np.zeros(X.shape[0])\n",
    "        probs[(scores>=0).flatten()] = 1/(1+np.exp(-1*scores[scores>=0].flatten()))\n",
    "        probs[(scores<0).flatten()] = 1 - 1/(1+np.exp(scores[scores<0].flatten()))\n",
    "        probs = np.hstack(((1-probs).reshape(X.shape[0], 1), probs.reshape(X.shape[0], 1)))\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class for each object in X\n",
    "        \n",
    "        Args:\n",
    "            X(ndarray):      objects\n",
    "        \n",
    "        Return:\n",
    "            pred(ndarray):   class for each object\n",
    "        \"\"\"\n",
    "        # get scores for each class\n",
    "        probs = probs = self.predict_probs(X)\n",
    "        # choose class with maximum score\n",
    "        pred = np.argmax(probs, axis=1)\n",
    "        return pred\n",
    "    \n",
    "    def get_statistics(self):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        df_coef_stats = pd.DataFrame(np.zeros((X.shape[1]+1, 6)), columns=['estimate', 'sd err', 'z_score', \n",
    "                                                        'p_value', 'lower', 'upper'])\n",
    "        df_model_stats = pd.DataFrame(np.zeros((1, 5)), columns=['logLikelihood', 'R_sqaured', 'adj_R_squared',\n",
    "                                                              'AIC', \"BIC\"])\n",
    "        \n",
    "        X_name = ['intercept']\n",
    "        for i in range(1, X.shape[1]+1):\n",
    "            X_name.append('X'+str(i))\n",
    "        df_coef_stats.index = X_name\n",
    "        \n",
    "        df_coef_stats['estimate'] = self._theta\n",
    "        df_coef_stats['sd err'] = self.sigma_estimates\n",
    "        df_coef_stats['z_score'] = self.z_scores\n",
    "        df_coef_stats['p_value'] = self.p_values\n",
    "        df_coef_stats['lower'] = self.lower\n",
    "        df_coef_stats['upper'] = self.upper\n",
    "        \n",
    "        df_model_stats.iloc[0, 0] = self.likelihood\n",
    "        df_model_stats.iloc[0, 1] = self.Rsquared\n",
    "        df_model_stats.iloc[0, 2] = self.adj_Rsquared\n",
    "        df_model_stats.iloc[0, 3] = self.AIC\n",
    "        df_model_stats.iloc[0, 4] = self.BIC\n",
    "        \n",
    "        return (df_model_stats, df_coef_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.3)\n",
    "matplotlib.rcParams[\"legend.framealpha\"] = 1\n",
    "matplotlib.rcParams[\"legend.frameon\"] = True\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's compare the homegrown method to the methods from sklearn and statsmodel**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary case\n",
    "\n",
    "0: non-versicolour and 1: versicolour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_bi_train = (y_train == 1).astype(int)\n",
    "y_bi_test = (y_test == 1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['intercept', 'X1', 'X2', 'X3', 'X4', 'train accuracy', 'test accuracy'])\n",
    "\n",
    "models = {'sklearn Logistic Regression Ovr': LogisticRegression(C=1e6, multi_class=\"ovr\", solver='liblinear'),\n",
    "         'sklearn Logistic Regression Multinomial': LogisticRegression(C=1e6, multi_class=\"multinomial\", solver='lbfgs'),\n",
    "         'sklearn SGD': SGDClassifier(loss='log', penalty='l2', alpha=1e-6, max_iter=10000, tol=1e-6)}\n",
    "\n",
    "for key, model in models.items():\n",
    "    model.fit(X_train, y_bi_train)\n",
    "    results = results.append(pd.DataFrame(\n",
    "            {'intercept': model.intercept_, 'X1': model.coef_[0, 0], \n",
    "             'X2':model.coef_[0, 1], 'X3':model.coef_[0, 2], 'X4':model.coef_[0, 3], \n",
    "             'train accuracy': np.round(accuracy_score(y_bi_train, model.predict(X_train)), 3), \n",
    "             'test accuracy': np.round(accuracy_score(y_bi_test, model.predict(X_test)), 3)},\n",
    "            index=[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>intercept</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>train accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sklearn Logistic Regression Ovr</th>\n",
       "      <td>0.048248</td>\n",
       "      <td>-2.906586</td>\n",
       "      <td>1.016315</td>\n",
       "      <td>-2.373328</td>\n",
       "      <td>6.691446</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sklearn Logistic Regression Multinomial</th>\n",
       "      <td>0.023765</td>\n",
       "      <td>-1.453643</td>\n",
       "      <td>0.508242</td>\n",
       "      <td>-1.186591</td>\n",
       "      <td>3.348442</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sklearn SGD</th>\n",
       "      <td>417.547623</td>\n",
       "      <td>-883.891077</td>\n",
       "      <td>280.851093</td>\n",
       "      <td>-739.327237</td>\n",
       "      <td>313.157542</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 X1          X2          X3  \\\n",
       "sklearn Logistic Regression Ovr            0.048248   -2.906586    1.016315   \n",
       "sklearn Logistic Regression Multinomial    0.023765   -1.453643    0.508242   \n",
       "sklearn SGD                              417.547623 -883.891077  280.851093   \n",
       "\n",
       "                                                 X4   intercept  \\\n",
       "sklearn Logistic Regression Ovr           -2.373328    6.691446   \n",
       "sklearn Logistic Regression Multinomial   -1.186591    3.348442   \n",
       "sklearn SGD                             -739.327237  313.157542   \n",
       "\n",
       "                                         test accuracy  train accuracy  \n",
       "sklearn Logistic Regression Ovr                  0.833           0.708  \n",
       "sklearn Logistic Regression Multinomial          0.833           0.708  \n",
       "sklearn SGD                                      0.633           0.692  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.492163\n",
      "         Iterations: 37\n",
      "         Function evaluations: 39\n",
      "         Gradient evaluations: 39\n"
     ]
    }
   ],
   "source": [
    "X_train_ = sm.add_constant(X_train)\n",
    "X_test_ = sm.add_constant(X_test)\n",
    "logit = Logit(y_bi_train, X_train_)\n",
    "res = logit.fit(method='bfgs', maxiter=5000)\n",
    "preds_train = res.predict(X_train_)\n",
    "preds_train = (preds_train >= 0.5).astype(int)\n",
    "preds_test = res.predict(X_test_)\n",
    "preds_test = (preds_test >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>   <td>0.234</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>              <td>AIC:</td>        <td>128.1190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2019-03-08 21:09</td>       <td>BIC:</td>        <td>142.0565</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>120</td>        <td>Log-Likelihood:</td>   <td>-59.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>4</td>            <td>LL-Null:</td>       <td>-77.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>115</td>            <td>Scale:</td>        <td>1.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>              <td></td>              <td></td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>           <th>Coef.</th>  <th>Std.Err.</th>    <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>             <td>6.6969</td>   <td>2.6304</td>  <td>2.5460</td>  <td>0.0109</td> <td>1.5414</td>  <td>11.8523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sepal length (cm)</th> <td>0.0475</td>   <td>0.6998</td>  <td>0.0679</td>  <td>0.9458</td> <td>-1.3240</td> <td>1.4190</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sepal width (cm)</th>  <td>-2.9073</td>  <td>0.8612</td>  <td>-3.3758</td> <td>0.0007</td> <td>-4.5952</td> <td>-1.2194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>petal length (cm)</th> <td>1.0165</td>   <td>0.7509</td>  <td>1.3538</td>  <td>0.1758</td> <td>-0.4551</td> <td>2.4882</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>petal width (cm)</th>  <td>-2.3732</td>  <td>1.3002</td>  <td>-1.8254</td> <td>0.0679</td> <td>-4.9215</td> <td>0.1750</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                         Results: Logit\n",
       "=================================================================\n",
       "Model:                Logit            Pseudo R-squared: 0.234   \n",
       "Dependent Variable:   y                AIC:              128.1190\n",
       "Date:                 2019-03-08 21:09 BIC:              142.0565\n",
       "No. Observations:     120              Log-Likelihood:   -59.060 \n",
       "Df Model:             4                LL-Null:          -77.056 \n",
       "Df Residuals:         115              Scale:            1.0000  \n",
       "Converged:            1.0000                                     \n",
       "-----------------------------------------------------------------\n",
       "                   Coef.  Std.Err.    z    P>|z|   [0.025  0.975]\n",
       "-----------------------------------------------------------------\n",
       "const              6.6969   2.6304  2.5460 0.0109  1.5414 11.8523\n",
       "sepal length (cm)  0.0475   0.6998  0.0679 0.9458 -1.3240  1.4190\n",
       "sepal width (cm)  -2.9073   0.8612 -3.3758 0.0007 -4.5952 -1.2194\n",
       "petal length (cm)  1.0165   0.7509  1.3538 0.1758 -0.4551  2.4882\n",
       "petal width (cm)  -2.3732   1.3002 -1.8254 0.0679 -4.9215  0.1750\n",
       "=================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = results.append(pd.DataFrame(\n",
    "        {'intercept': res.params[0], 'X1': res.params[1], \n",
    "         'X2':res.params[2], 'X3':res.params[3], 'X4':res.params[4], \n",
    "         'train accuracy': np.round(accuracy_score(y_bi_train, preds_train), 3), \n",
    "         'test accuracy': np.round(accuracy_score(y_bi_test, preds_test), 3)},\n",
    "        index=['statsmodel Logit']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>intercept</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>train accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sklearn Logistic Regression Ovr</th>\n",
       "      <td>0.048248</td>\n",
       "      <td>-2.906586</td>\n",
       "      <td>1.016315</td>\n",
       "      <td>-2.373328</td>\n",
       "      <td>6.691446</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sklearn Logistic Regression Multinomial</th>\n",
       "      <td>0.023765</td>\n",
       "      <td>-1.453643</td>\n",
       "      <td>0.508242</td>\n",
       "      <td>-1.186591</td>\n",
       "      <td>3.348442</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sklearn SGD</th>\n",
       "      <td>417.547623</td>\n",
       "      <td>-883.891077</td>\n",
       "      <td>280.851093</td>\n",
       "      <td>-739.327237</td>\n",
       "      <td>313.157542</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statsmodel Logit</th>\n",
       "      <td>0.047534</td>\n",
       "      <td>-2.907298</td>\n",
       "      <td>1.016506</td>\n",
       "      <td>-2.373235</td>\n",
       "      <td>6.696884</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 X1          X2          X3  \\\n",
       "sklearn Logistic Regression Ovr            0.048248   -2.906586    1.016315   \n",
       "sklearn Logistic Regression Multinomial    0.023765   -1.453643    0.508242   \n",
       "sklearn SGD                              417.547623 -883.891077  280.851093   \n",
       "statsmodel Logit                           0.047534   -2.907298    1.016506   \n",
       "\n",
       "                                                 X4   intercept  \\\n",
       "sklearn Logistic Regression Ovr           -2.373328    6.691446   \n",
       "sklearn Logistic Regression Multinomial   -1.186591    3.348442   \n",
       "sklearn SGD                             -739.327237  313.157542   \n",
       "statsmodel Logit                          -2.373235    6.696884   \n",
       "\n",
       "                                         test accuracy  train accuracy  \n",
       "sklearn Logistic Regression Ovr                  0.833           0.708  \n",
       "sklearn Logistic Regression Multinomial          0.833           0.708  \n",
       "sklearn SGD                                      0.633           0.692  \n",
       "statsmodel Logit                                 0.833           0.708  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## homegrown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_wlr = WeightedBinaryLogisticRegression()\n",
    "model_wlr.fit(X_train, y_bi_train, max_iter=50000, eta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = results.append(pd.DataFrame(\n",
    "        {'intercept': model_wlr._theta[0], 'X1': model_wlr._theta[1], \n",
    "         'X2':model_wlr._theta[2], 'X3':model_wlr._theta[3], 'X4':model_wlr._theta[4], \n",
    "         'train accuracy': np.round(accuracy_score(y_bi_train, model_wlr.predict(X_train)), 3), \n",
    "         'test accuracy': np.round(accuracy_score(y_bi_test, model_wlr.predict(X_test)), 3)},\n",
    "        index=['Homegrown Weighted Binary Logisitc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1, df2 = model_wlr.get_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare the model statistics from homegrown method to statsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logLikelihood</th>\n",
       "      <th>R_sqaured</th>\n",
       "      <th>adj_R_squared</th>\n",
       "      <th>AIC</th>\n",
       "      <th>BIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-59.059529</td>\n",
       "      <td>0.233552</td>\n",
       "      <td>0.199936</td>\n",
       "      <td>128.119057</td>\n",
       "      <td>142.056516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   logLikelihood  R_sqaured  adj_R_squared         AIC         BIC\n",
       "0     -59.059529   0.233552       0.199936  128.119057  142.056516"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>sd err</th>\n",
       "      <th>z_score</th>\n",
       "      <th>p_value</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>6.679428</td>\n",
       "      <td>2.629149</td>\n",
       "      <td>2.540529</td>\n",
       "      <td>0.011069</td>\n",
       "      <td>1.526391</td>\n",
       "      <td>11.832465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X1</th>\n",
       "      <td>0.049510</td>\n",
       "      <td>0.699633</td>\n",
       "      <td>0.070765</td>\n",
       "      <td>0.943585</td>\n",
       "      <td>-1.321746</td>\n",
       "      <td>1.420765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X2</th>\n",
       "      <td>-2.904532</td>\n",
       "      <td>0.860787</td>\n",
       "      <td>-3.374276</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>-4.591642</td>\n",
       "      <td>-1.217421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X3</th>\n",
       "      <td>1.015781</td>\n",
       "      <td>0.750680</td>\n",
       "      <td>1.353148</td>\n",
       "      <td>0.176008</td>\n",
       "      <td>-0.455525</td>\n",
       "      <td>2.487086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X4</th>\n",
       "      <td>-2.372817</td>\n",
       "      <td>1.299877</td>\n",
       "      <td>-1.825417</td>\n",
       "      <td>0.067938</td>\n",
       "      <td>-4.920529</td>\n",
       "      <td>0.174894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           estimate    sd err   z_score   p_value     lower      upper\n",
       "intercept  6.679428  2.629149  2.540529  0.011069  1.526391  11.832465\n",
       "X1         0.049510  0.699633  0.070765  0.943585 -1.321746   1.420765\n",
       "X2        -2.904532  0.860787 -3.374276  0.000740 -4.591642  -1.217421\n",
       "X3         1.015781  0.750680  1.353148  0.176008 -0.455525   2.487086\n",
       "X4        -2.372817  1.299877 -1.825417  0.067938 -4.920529   0.174894"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare the results from all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>intercept</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>train accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sklearn Logistic Regression Ovr</th>\n",
       "      <td>0.048248</td>\n",
       "      <td>-2.906586</td>\n",
       "      <td>1.016315</td>\n",
       "      <td>-2.373328</td>\n",
       "      <td>6.691446</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sklearn Logistic Regression Multinomial</th>\n",
       "      <td>0.023765</td>\n",
       "      <td>-1.453643</td>\n",
       "      <td>0.508242</td>\n",
       "      <td>-1.186591</td>\n",
       "      <td>3.348442</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sklearn SGD</th>\n",
       "      <td>417.547623</td>\n",
       "      <td>-883.891077</td>\n",
       "      <td>280.851093</td>\n",
       "      <td>-739.327237</td>\n",
       "      <td>313.157542</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statsmodel Logit</th>\n",
       "      <td>0.047534</td>\n",
       "      <td>-2.907298</td>\n",
       "      <td>1.016506</td>\n",
       "      <td>-2.373235</td>\n",
       "      <td>6.696884</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Homegrown Weighted Binary Logisitc</th>\n",
       "      <td>0.049510</td>\n",
       "      <td>-2.904532</td>\n",
       "      <td>1.015781</td>\n",
       "      <td>-2.372817</td>\n",
       "      <td>6.679428</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 X1          X2          X3  \\\n",
       "sklearn Logistic Regression Ovr            0.048248   -2.906586    1.016315   \n",
       "sklearn Logistic Regression Multinomial    0.023765   -1.453643    0.508242   \n",
       "sklearn SGD                              417.547623 -883.891077  280.851093   \n",
       "statsmodel Logit                           0.047534   -2.907298    1.016506   \n",
       "Homegrown Weighted Binary Logisitc         0.049510   -2.904532    1.015781   \n",
       "\n",
       "                                                 X4   intercept  \\\n",
       "sklearn Logistic Regression Ovr           -2.373328    6.691446   \n",
       "sklearn Logistic Regression Multinomial   -1.186591    3.348442   \n",
       "sklearn SGD                             -739.327237  313.157542   \n",
       "statsmodel Logit                          -2.373235    6.696884   \n",
       "Homegrown Weighted Binary Logisitc        -2.372817    6.679428   \n",
       "\n",
       "                                         test accuracy  train accuracy  \n",
       "sklearn Logistic Regression Ovr                  0.833           0.708  \n",
       "sklearn Logistic Regression Multinomial          0.833           0.708  \n",
       "sklearn SGD                                      0.633           0.692  \n",
       "statsmodel Logit                                 0.833           0.708  \n",
       "Homegrown Weighted Binary Logisitc               0.833           0.708  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "          random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "          warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn_model_sklearnLogistic = LogisticRegression(C=1e6, multi_class=\"multinomial\", solver='lbfgs')\n",
    "mn_model_sklearnLogistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_mn = []\n",
    "result_mn.append(pd.DataFrame(np.hstack((mn_model_sklearnLogistic.intercept_.reshape((3, 1)), \n",
    "                                  mn_model_sklearnLogistic.coef_)).T, columns=['setosa', 'versicolour', 'virginica']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_acc = pd.DataFrame(columns=['train accuracy', 'test accuracy'])\n",
    "result_acc = result_acc.append(pd.DataFrame({'train accuracy':accuracy_score(y_train, mn_model_sklearnLogistic.predict(X_train)),\n",
    "                        'test accuracy':accuracy_score(y_test, mn_model_sklearnLogistic.predict(X_test)) }, \n",
    "                    index=['sklearn_logistic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1e-06, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=10000,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=1e-06,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn_model_SGD = SGDClassifier(loss='log', penalty='l2', alpha=1e-6, max_iter=10000, tol=1e-6)\n",
    "mn_model_SGD.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_mn.append(pd.DataFrame(np.hstack((mn_model_SGD.intercept_.reshape((3, 1)), \n",
    "                                  mn_model_SGD.coef_)).T, columns=['setosa', 'versicolour', 'virginica']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_acc = result_acc.append(pd.DataFrame({'train accuracy':accuracy_score(y_train, mn_model_SGD.predict(X_train)),\n",
    "                        'test accuracy':accuracy_score(y_test, mn_model_SGD.predict(X_test))}, \n",
    "                    index=['sklearn_sgd']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.046578\n",
      "         Iterations: 64\n",
      "         Function evaluations: 66\n",
      "         Gradient evaluations: 66\n"
     ]
    }
   ],
   "source": [
    "mnlogit = sm.MNLogit(y_train, X_train_)\n",
    "mn_res = mnlogit.fit(method='bfgs', maxiter=5000)\n",
    "preds_train_ = mn_res.predict(X_train_)\n",
    "preds_train_ = np.argmax(preds_train_.values, axis=1)\n",
    "preds_test_ = mn_res.predict(X_test_)\n",
    "preds_test_ = np.argmax(preds_test_.values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_mn.append(pd.DataFrame(mn_res.params.values, columns=['setosa', 'versicolour']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_acc = result_acc.append(pd.DataFrame({'train accuracy':accuracy_score(y_train, preds_train_),\n",
    "                        'test accuracy':accuracy_score(y_test, preds_test_)}, \n",
    "                    index=['statsmodel MNLogit']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## homegrown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "# using raw gradient descent seems to  be really slow\n",
    "# need to implement lbfgs or other algorithm for speed\n",
    "%%time\n",
    "mn_model_wlr = WeightedMultinomialLogisticRegression()\n",
    "mn_model_wlr.fit(X_train, y_train, max_iter=100000, eta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.91051048,  8.71272432, -8.99897806],\n",
       "       [ 3.08943081,  0.86441483, -1.62730551],\n",
       "       [ 6.07475898,  0.99602217, -5.19701774],\n",
       "       [-6.58229067,  1.04687124,  7.38370817],\n",
       "       [-3.44244263, -3.93823869,  7.85785818]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn_model_wlr._theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_mn.append(pd.DataFrame(mn_model_wlr._theta, columns=['setosa', 'versicolour', 'virginica']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_coef_mn = pd.concat(result_mn, keys=['sklearn_logistic', 'sklearn_sgd', 'statsmodel', 'homegrown'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_acc = result_acc.append(pd.DataFrame({'train accuracy':accuracy_score(y_train, mn_model_wlr.predict(X_train)),\n",
    "                        'test accuracy':accuracy_score(y_test, mn_model_wlr.predict(X_test))}, \n",
    "                    index=['homegrown']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare the fitted coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">sklearn_logistic</th>\n",
       "      <th colspan=\"3\" halign=\"left\">sklearn_sgd</th>\n",
       "      <th colspan=\"2\" halign=\"left\">statsmodel</th>\n",
       "      <th colspan=\"3\" halign=\"left\">homegrown</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolour</th>\n",
       "      <th>virginica</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolour</th>\n",
       "      <th>virginica</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolour</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolour</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>2.206041</td>\n",
       "      <td>16.730197</td>\n",
       "      <td>-18.936238</td>\n",
       "      <td>15.808389</td>\n",
       "      <td>292.531497</td>\n",
       "      <td>-925.212410</td>\n",
       "      <td>14.605506</td>\n",
       "      <td>-21.068110</td>\n",
       "      <td>1.910510</td>\n",
       "      <td>8.712724</td>\n",
       "      <td>-8.998978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X1</th>\n",
       "      <td>4.322002</td>\n",
       "      <td>-1.094362</td>\n",
       "      <td>-3.227641</td>\n",
       "      <td>18.483277</td>\n",
       "      <td>486.386042</td>\n",
       "      <td>-1167.397472</td>\n",
       "      <td>-5.278503</td>\n",
       "      <td>-7.411876</td>\n",
       "      <td>3.089431</td>\n",
       "      <td>0.864415</td>\n",
       "      <td>-1.627306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X2</th>\n",
       "      <td>9.933641</td>\n",
       "      <td>-1.536074</td>\n",
       "      <td>-8.397567</td>\n",
       "      <td>81.634472</td>\n",
       "      <td>-1019.513567</td>\n",
       "      <td>-1320.851814</td>\n",
       "      <td>-12.643246</td>\n",
       "      <td>-19.505523</td>\n",
       "      <td>6.074759</td>\n",
       "      <td>0.996022</td>\n",
       "      <td>-5.197018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X3</th>\n",
       "      <td>-14.528462</td>\n",
       "      <td>3.109692</td>\n",
       "      <td>11.418770</td>\n",
       "      <td>-98.577476</td>\n",
       "      <td>348.470155</td>\n",
       "      <td>1880.543948</td>\n",
       "      <td>18.475378</td>\n",
       "      <td>26.785759</td>\n",
       "      <td>-6.582291</td>\n",
       "      <td>1.046871</td>\n",
       "      <td>7.383708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X4</th>\n",
       "      <td>-6.881934</td>\n",
       "      <td>-4.788829</td>\n",
       "      <td>11.670762</td>\n",
       "      <td>-38.506827</td>\n",
       "      <td>-653.107398</td>\n",
       "      <td>1710.828316</td>\n",
       "      <td>2.141630</td>\n",
       "      <td>18.603335</td>\n",
       "      <td>-3.442443</td>\n",
       "      <td>-3.938239</td>\n",
       "      <td>7.857858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sklearn_logistic                        sklearn_sgd               \\\n",
       "                    setosa versicolour  virginica      setosa  versicolour   \n",
       "intercept         2.206041   16.730197 -18.936238   15.808389   292.531497   \n",
       "X1                4.322002   -1.094362  -3.227641   18.483277   486.386042   \n",
       "X2                9.933641   -1.536074  -8.397567   81.634472 -1019.513567   \n",
       "X3              -14.528462    3.109692  11.418770  -98.577476   348.470155   \n",
       "X4               -6.881934   -4.788829  11.670762  -38.506827  -653.107398   \n",
       "\n",
       "                       statsmodel             homegrown                        \n",
       "             virginica     setosa versicolour    setosa versicolour virginica  \n",
       "intercept  -925.212410  14.605506  -21.068110  1.910510    8.712724 -8.998978  \n",
       "X1        -1167.397472  -5.278503   -7.411876  3.089431    0.864415 -1.627306  \n",
       "X2        -1320.851814 -12.643246  -19.505523  6.074759    0.996022 -5.197018  \n",
       "X3         1880.543948  18.475378   26.785759 -6.582291    1.046871  7.383708  \n",
       "X4         1710.828316   2.141630   18.603335 -3.442443   -3.938239  7.857858  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_coef_mn.index = ['intercept', 'X1', 'X2', 'X3', 'X4']\n",
    "result_coef_mn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>train accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sklearn_logistic</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sklearn_sgd</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statsmodel MNLogit</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homegrown</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    test accuracy  train accuracy\n",
       "sklearn_logistic         1.000000        0.983333\n",
       "sklearn_sgd              0.866667        0.850000\n",
       "statsmodel MNLogit       1.000000        0.983333\n",
       "homegrown                1.000000        0.983333"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
